{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maaz Ali\\AppData\\Local\\Temp\\ipykernel_18028\\3516484815.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tqdm().pandas()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ad6f519c5548b7924e776a81a6a129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import os, time\n",
    "from PIL import Image\n",
    "from pickle import dump, load\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical, get_file\n",
    "from keras.layers import add, Input, Dense, LSTM, Embedding, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# small library for seeing the progress of loops.\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Files\n",
    "def load_file(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all images with their Captions\n",
    "def all_img_captions(filename):\n",
    "    file = load_file(filename)\n",
    "    captions = file.split('\\n')\n",
    "    descriptions = {}\n",
    "    for caption in captions[:-1]:\n",
    "        img, caption = caption.split('\\t')\n",
    "        if img[:-2] not in descriptions:\n",
    "            descriptions[img[:-2]] = [ caption ]\n",
    "        else:\n",
    "            descriptions[img[:-2]].append(caption)\n",
    "    return descriptions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning- lower casing, removing puntuations and words containing numbers\n",
    "def cleaning_text(captions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for img, caps in captions.items():\n",
    "        for i, img_caption in enumerate(caps):\n",
    "            img_caption = img_caption.replace(\"-\",\"\")\n",
    "            desc = img_caption.split()\n",
    "            #converts to lowercase\n",
    "            desc = [word.lower() for word in desc]\n",
    "            #remove punctuation from each token\n",
    "            desc = [word.translate(table) for word in desc]\n",
    "            #remove hanging 's and a \n",
    "            desc = [word for word in desc if(len(word)>1)]\n",
    "            #remove tokens with numbers in them\n",
    "            desc = [word for word in desc if(word.isalpha())]\n",
    "            #convert back to string\n",
    "\n",
    "            img_caption = ' '.join(desc)\n",
    "            captions[img][i] = img_caption\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary of all unique words\n",
    "def text_vocabolary(descriptions):\n",
    "    vocab = set()\n",
    "    for key in descriptions.keys():\n",
    "        [vocab.update(d.split()) for d in descriptions[key]]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save all descriptions in one file \n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        lines.append(key + '\\t' + ' '.join(desc_list))\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these path according to project folder in you system\n",
    "dataset_text = \"D:\\DataCamp Projects\\LLM\\Flickr8k_text\"\n",
    "dataset_images = \"D:\\DataCamp Projects\\LLM\\Flickr8k_Dataset\\Flicker8k_Dataset\"\n",
    "\n",
    "#we prepare our text data\n",
    "#filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n",
    "#loading the file that contains all data\n",
    "#mapping them into descriptions dictionary img to 5 captions\n",
    "#descriptions = all_img_captions(filename)\n",
    "#print(\"Length of descriptions =\" ,len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the descriptions\n",
    "#clean_descriptions = cleaning_text(descriptions)\n",
    "\n",
    "#Building Vocabolary\n",
    "#vocabolary = text_vocabolary(clean_descriptions)\n",
    "#print(\"Length of Vocabolary = \", len(vocabolary))\n",
    "\n",
    "#Saving each description to file \n",
    "#save_descriptions(clean_descriptions, \"descriptions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download a model\n",
    "def download_with_retry(url, filename, max_retries = 3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return get_file(filename, url)\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries -1:\n",
    "                raise e\n",
    "            print(f\"Download attempt failed\")\n",
    "            time.sleep(3)\n",
    "\n",
    "weights_url = \"https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "weights_path = download_with_retry(weights_url, 'xception_weights.h5')\n",
    "\n",
    "#model = Xception(include_top=False, pooling=\"avg\", weights=weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Features from Images\n",
    "def extract_features(directory):\n",
    "    features = {}\n",
    "    valid_images = ['.jpg', 'jpeg', '.png']\n",
    "    for img in tqdm(os.listdir(directory)):\n",
    "        ext = os.path.splitext(img)[1].lower()\n",
    "        if ext not in valid_images:\n",
    "            continue\n",
    "        filename = directory + \"/\" + img\n",
    "        image = Image.open(filename)\n",
    "        image = image.resize((299,299))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = image/127.5\n",
    "        image = image - 1.0\n",
    "\n",
    "        feature = model.predict(image)\n",
    "        features[img] = feature\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2048 Feature Vector\n",
    "#features = extract_features(dataset_images)\n",
    "#dump(features, open(\"features.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load(open(\"features.p\", 'rb'))\n",
    "\n",
    "#Load the Data\n",
    "def load_photos(filename):\n",
    "    file = load_file(filename)\n",
    "    photos = file.split(\"\\n\")[:-1]\n",
    "    photos_present = [photo for photo in photos if os.path.exists(os.path.join(dataset_images, photo))]\n",
    "    return photos_present\n",
    "\n",
    "def load_clean_descriptions(filename, photos):\n",
    "    file = load_file(filename)\n",
    "    descriptions = {}\n",
    "    for line in file.split(\"\\n\"):\n",
    "        words = line.split()\n",
    "        if len(words) < 1:\n",
    "            continue\n",
    "\n",
    "        image, image_caption = words[0], words[1:]\n",
    "        if image in photos:\n",
    "            if image not in descriptions:\n",
    "                descriptions[image] = []\n",
    "            desc = '<start>' + \" \".join(image_caption) + '<end>'\n",
    "            descriptions[image].append(desc)\n",
    "\n",
    "    return descriptions\n",
    "\n",
    "def load_features(photos):\n",
    "    all_features = load(open(\"features.p\", \"rb\"))\n",
    "    features = {k:all_features[k] for k in photos}\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
    "\n",
    "train_imgs = load_photos(filename)\n",
    "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
    "train_features = load_features(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting dictionary to clean list of descriptions\n",
    "def dict_to_list(descriptions):\n",
    "    all_desc = []\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "#creating tokenizer class \n",
    "#this will vectorise text corpus\n",
    "#each integer will represent token in dictionary\n",
    "def create_tokenizer(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(desc_list)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give each word an index, and store that into tokenizer.p pickle file\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "#dump(tokenizer, open(\"tokenizer.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7577\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "#calculate maximum length of descriptions\n",
    "def max_length(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    return max(len(d.split()) for d in desc_list)\n",
    "    \n",
    "max_length = max_length(train_descriptions)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input-output sequence pairs from the image description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Sequences\n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "        \n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generator, used by model.fit()\n",
    "def data_generator(descriptions, features, tokenizer, max_length):\n",
    "    def generator():\n",
    "        for key, descriptions_list in descriptions.items():\n",
    "            while True:\n",
    "                feature = features[key]\n",
    "                input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, descriptions_list, feature)\n",
    "                for i in range(len(input_image)):\n",
    "                    yield {'input_1': input_image[i].squeeze(), 'input_2': input_sequence[i]}, output_word[i]\n",
    "\n",
    "    # Define the output signature for the generator\n",
    "    output_signature = (\n",
    "        {\n",
    "            'input_1': tf.TensorSpec(shape=(2048,), dtype = (tf.float32)),\n",
    "            'input_2': tf.TensorSpec(shape=(max_length,), dtype= (tf.int32))\n",
    "        },\n",
    "        tf.TensorSpec(shape=(vocab_size,), dtype=(tf.float32))\n",
    "    )\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "\n",
    "    return dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the shape of the input and output for your model\n",
    "#dataset = data_genertor(train_descriptions, features, tokenizer, max_length)\n",
    "#for (a, b) in dataset.take(1):\n",
    "#    print(a['input_1'].shape, a['input_2'].shape, b.shape)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "\n",
    "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
    "    inputs1 = Input(shape=(2048,), name='input_1')\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # LSTM sequence model\n",
    "    inputs2 = Input(shape=(max_length,), name='input_2')\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # Merging both models\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], run_eagerly=True)\n",
    "\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train= 6000\n",
      "Photos: train= 6000\n",
      "Vocabulary Size: 7577\n",
      "Description Length:  86\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 86)]         0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 86, 256)      1939712     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2048)         0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 86, 256)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          524544      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          525312      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 7577)         1947289     ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,002,649\n",
      "Trainable params: 5,002,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Steps per epoch:  8450\n",
      "Sample data: ({'input_1': <tf.Tensor: shape=(32, 2048), dtype=float32, numpy=\n",
      "array([[0.47340965, 0.01730897, 0.07334232, ..., 0.08557957, 0.02102298,\n",
      "        0.23765525],\n",
      "       [0.47340965, 0.01730897, 0.07334232, ..., 0.08557957, 0.02102298,\n",
      "        0.23765525],\n",
      "       [0.47340965, 0.01730897, 0.07334232, ..., 0.08557957, 0.02102298,\n",
      "        0.23765525],\n",
      "       ...,\n",
      "       [0.47340965, 0.01730897, 0.07334232, ..., 0.08557957, 0.02102298,\n",
      "        0.23765525],\n",
      "       [0.47340965, 0.01730897, 0.07334232, ..., 0.08557957, 0.02102298,\n",
      "        0.23765525],\n",
      "       [0.47340965, 0.01730897, 0.07334232, ..., 0.08557957, 0.02102298,\n",
      "        0.23765525]], dtype=float32)>, 'input_2': <tf.Tensor: shape=(32, 86), dtype=int32, numpy=\n",
      "array([[  0,   0,   0, ...,   0,   0,   8],\n",
      "       [  0,   0,   0, ...,   0,   8,  42],\n",
      "       [  0,   0,   0, ...,   8,  42,   1],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,  18, 117,   2],\n",
      "       [  0,   0,   0, ..., 117,   2, 394],\n",
      "       [  0,   0,   0, ...,   2, 394,  19]])>}, <tf.Tensor: shape=(32, 7577), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>)\n",
      "Epoch 1/4\n",
      " 914/8450 [==>...........................] - ETA: 3:22:04 - loss: 0.5507 - accuracy: 0.8600"
     ]
    }
   ],
   "source": [
    "# train our model\n",
    "print('Dataset: ', len(train_imgs))\n",
    "print('Descriptions: train=', len(train_descriptions))\n",
    "print('Photos: train=', len(train_features))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length: ', max_length)\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 10\n",
    "\n",
    "def get_steps_per_epoch(train_descriptions):\n",
    "    total_sequences = 0\n",
    "    for img_captions in train_descriptions.values():\n",
    "        for caption in img_captions:\n",
    "            words = caption.split()\n",
    "            total_sequences += len(words) - 1\n",
    "    # Ensure at least 1 step, even if sequences < batch_size\n",
    "    return max(1, total_sequences // 32)\n",
    "\n",
    "# Update training loop\n",
    "steps = get_steps_per_epoch(train_descriptions)\n",
    "print(\"Steps per epoch: \", steps)  # Debugging line\n",
    "\n",
    "# making a directory models to save our models\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "for i in range(epochs):\n",
    "    dataset = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "    # Verify dataset is not empty\n",
    "    for data in dataset.take(1):  # Take one batch to inspect it\n",
    "        print(\"Sample data:\", data)\n",
    "\n",
    "    model.fit(dataset, epochs=4, steps_per_epoch=steps, verbose=1)\n",
    "    model.save(f\"models/model_{i}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
